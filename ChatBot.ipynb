{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvW1T3VDWHWb"
      },
      "source": [
        "### Intro: \n",
        "\n",
        "In This Project we will implement a Deep NLP ChatBot using Tensorflow.\n",
        "\n",
        "We'll start by importing the libraries needed for this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqaWJznwWHWk"
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import re #Helps with data preprocessing, with rgular epressions to be exact\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iydHsd17WHWk",
        "outputId": "29d3ea2b-d7e4-43a4-ee7b-f0b22ad7c23c"
      },
      "source": [
        "# Our version of Tensorflow is 1.0.0\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le36FBkhWHWm"
      },
      "source": [
        "The dataset used for the training of this ChatBot are taking from: \n",
        "\n",
        "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html\n",
        "\n",
        "This dataset is called Cornell Movie--Dialogs Corpus, and it contains conversations between actors from a large number of movies, so the type of our ChatBot would be a friend-like ChatBot (able to do casual conversations), for more field specific ChatBots we can use other kind of datasets. Anyway, for further informations about the data used you can look at the link above.\n",
        "\n",
        "It's important to know that the dataset used is composed of 2 text files: \"movie_lines.txt\" and \"movie_conversations.txt\". The first contains the lines from different movies in an unorderly fashion, but these lines have IDs, these IDs are used in the second file to identify the lines that correspond to a certain conversation, so the second file works as a way to order the line from the first file. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwAl1FSoWHWm"
      },
      "source": [
        "# I. Data preprocesing: \n",
        "\n",
        "Generally, this is the longest part of each project, in which we will make the data ready for input into the deep learning model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfjsTcSMWHWn"
      },
      "source": [
        "# Loading data: We will load both the lines and conversations\n",
        "\n",
        "with open(\"C:/Users/YsfDS/Desktop/data/movie_lines.txt\",encoding='utf-8',errors='ignore') as f1:\n",
        "    lines=f1.read().split('\\n') #304714 lines\n",
        "with open(\"C:/Users/YsfDS/Desktop/data/movie_conversations.txt\",encoding='utf-8',errors='ignore') as f2:\n",
        "    convos=f2.read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwOTU5syWHWn"
      },
      "source": [
        "# Now let's create a dictionary that maps each line with its ID.\n",
        "id2line={}\n",
        "\n",
        "for line in lines:\n",
        "    spl=line.split(' +++$+++ ')\n",
        "    if len(spl)==5:\n",
        "        id2line[spl[0]]=spl[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhODe6bAWHWn"
      },
      "source": [
        "# We will now create a list of conversations. (IDs of lines in list)\n",
        "\n",
        "convoli= []\n",
        "\n",
        "for conv in convos[:-1]: #The last row of this list is empty\n",
        "    spl=conv.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(' ','')\n",
        "    convoli.append(spl.split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyWJDBOgWHWo"
      },
      "source": [
        "# From the list of convo ids we will try build two lists one for 'questions' and the other for 'answers'.\n",
        "\n",
        "questions=[]\n",
        "answers=[]\n",
        "\n",
        "for conv in convoli:\n",
        "    k=len(conv)\n",
        "    for i in range(k-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qJ1Q9oKWHWo"
      },
      "source": [
        "# Now for text cleaning\n",
        "\n",
        "def cleanText(text):\n",
        "    # text to lower case\n",
        "    text=text.lower()\n",
        "    # Now to make it easier for the ChatBot to learn we gonna use re to replace expression like \"i'm\" with \"i am\"\n",
        "    text=re.sub(r\"i'm\",\"i am\",text)\n",
        "    text=re.sub(r\"she's\",\"she is\",text)\n",
        "    text=re.sub(r\"he's\",\"he is\",text)\n",
        "    text=re.sub(r\"it's\",\"it is\",text)\n",
        "    text=re.sub(r\"that's\",\"that is\",text)\n",
        "    text=re.sub(r\"what's\",\"what is\",text)\n",
        "    text=re.sub(r\"where's\",\"where is\",text)\n",
        "    text=re.sub(r\"\\'ve\",\" have\",text)\n",
        "    text=re.sub(r\"\\'ll\",\" will\",text)\n",
        "    text=re.sub(r\"\\'d\",\" would\",text)\n",
        "    text=re.sub(r\"\\'re\",\" are\",text)\n",
        "    text=re.sub(r\"won't\",\"will not\",text)\n",
        "    text=re.sub(r\"can't\",\"cannot\",text)\n",
        "    text=re.sub(r\"n't\",\" not\",text)\n",
        "    text=re.sub(r\"[-()/\\\"#$%^&*()_+@=?<>:;,.!{}'|]\",\"\",text)\n",
        "    text=[word for word in text.split() if word.isalpha()]\n",
        "    text=' '.join(text)\n",
        "    #Do as you can in here the better the cleaning the better the result\n",
        "    return(text)\n",
        "\n",
        "clean_questions=[cleanText(line) for line in questions]\n",
        "clean_answers=[cleanText(line) for line in answers]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z-5LQ5qWHWo"
      },
      "source": [
        "# Let's now remove qst/ans that are too long or too short.\n",
        "\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwoWo2ygWHWp"
      },
      "source": [
        "# In order to optimize our ChatBot training we will try to remove infrequent words from both questions and answers lists.\n",
        "# So the first step to do that is to generate a dictionnary that maps word to their cardinality within the dataset.\n",
        "\n",
        "wordOccur={}\n",
        "for question in clean_questions:\n",
        "    l=question.split()\n",
        "    for i in range (len(l)) :\n",
        "        if l[i] in wordOccur.keys():\n",
        "            wordOccur[l[i]]+=1\n",
        "        else:\n",
        "            wordOccur[l[i]]=1\n",
        "for answer in clean_answers:\n",
        "    l=answer.split()\n",
        "    for i in range (len(l)) :\n",
        "        if l[i] in wordOccur.keys():\n",
        "            wordOccur[l[i]]+=1\n",
        "        else:\n",
        "            wordOccur[l[i]]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08bIwZViWHWp"
      },
      "source": [
        "# The second step is to set a threshold for the number of occurence of words that will be used in the training of the model.\n",
        "# Let's create 2 dictionaries that map each word from questions/answers to a unique identifier.\n",
        "\n",
        "threshold=15 #This as of now a hyperparameter of the model, 20 seems reasonable we can either decrease it or increase it based on obtained results.\n",
        "\n",
        "Qwords=[q.split()[i] for q in clean_questions for i in range(len(q.split()))] #Words in the questions.\n",
        "Qwords=list(set(Qwords)) #Remove redundencies\n",
        "Awords=[a.split()[i] for a in clean_answers for i in range(len(a.split()))]   #Words in the answers.\n",
        "Awords=list(set(Awords))\n",
        "\n",
        "questionwordsIDs={}\n",
        "\n",
        "wordID=0\n",
        "for word , count in wordOccur.items():\n",
        "    if (count > threshold and word in Qwords):\n",
        "        questionwordsIDs[word]=wordID\n",
        "        wordID+=1\n",
        "        \n",
        "answerwordsIDs={}\n",
        "        \n",
        "wordID=0\n",
        "for word , count in wordOccur.items():\n",
        "    if (count > threshold and word in Awords):\n",
        "        answerwordsIDs[word]=wordID\n",
        "        wordID+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fMDgSd6WHWp"
      },
      "source": [
        "# We will now add tokens necessary for the SEQ2SEQ model to the dictionary with their unique IDs.\n",
        "\n",
        "tokens=['<PAD>','<EOS>','<OUT>','<SOS>']\n",
        "for token in tokens:\n",
        "    questionwordsIDs[token]=len(questionwordsIDs)+1\n",
        "for token in tokens:\n",
        "    answerwordsIDs[token]=len(answerwordsIDs)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j745-8MAWHWq"
      },
      "source": [
        "# In the implmentation of the SEQ2SEQ model we will need the inverse mapping ID--> word for the answer dictionary so let's do that.\n",
        "\n",
        "answerIDs2words={wordID:word for word,wordID in answerwordsIDs.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epydrCFhWHWq"
      },
      "source": [
        "# Let's add at the end to clean_answers <EOS>.\n",
        "\n",
        "for i in range (len(clean_answers)):\n",
        "    clean_answers[i]+=' <EOS>'\n",
        "\n",
        "# Now we will translate questions and answers into a set of integers which are their IDs as defined before.\n",
        "\n",
        "codedQuestions=[]\n",
        "i=0\n",
        "for question in clean_questions:\n",
        "    l=question.split()\n",
        "    temp=[]\n",
        "    if len(l)>0:\n",
        "        for word in l:\n",
        "            if (word not in questionwordsIDs.keys()):\n",
        "                temp.append(questionwordsIDs['<OUT>'])\n",
        "            else:\n",
        "                temp.append(questionwordsIDs[word])\n",
        "        codedQuestions.append(temp)\n",
        "        i+=1\n",
        "\n",
        "codedAnswers=[]\n",
        "for answer in clean_answers:\n",
        "    l=answer.split()\n",
        "    temp=[]\n",
        "    if len(l)>0:\n",
        "        for word in l:\n",
        "            if (word not in answerwordsIDs.keys()):\n",
        "                temp.append(answerwordsIDs['<OUT>'])\n",
        "            else:\n",
        "                temp.append(answerwordsIDs[word])\n",
        "        codedAnswers.append(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdcI0rUKWHWq"
      },
      "source": [
        "# So final step,  before getting into modeling and what we will need to do is sorting the questions and answers by length\n",
        "# this helps (speed-up) with the learning process. \n",
        "\n",
        "SortclAns = []\n",
        "SortclQues = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(codedQuestions):\n",
        "        if len(i[1]) == length:\n",
        "            SortclQues.append(codedQuestions[i[0]])\n",
        "            SortclAns.append(codedAnswers[i[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XZBCLPBWHWr"
      },
      "source": [
        "# II. Building the SEQ2SEQ model:\n",
        "\n",
        "Now we will start using Tensorflow to build the architecture of the model that ww will train in the next phase, so let's get into it.\n",
        "\n",
        "It's important to note that in Tensorflow all variables are tensors, a tensor is a special data structure that is without being mathematically rigorous can be considered as a multidimensional vector, a matrix for example is a rank 2 tensor. These tensor based variables allow a fast computation for deep neural networks, so in order to use this tensor variables we must define them in a Tensorflow placeholder. So the first thing we will do is create placeholders for inputs and targets and also for some hyperparameters. Let's go!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6AaGP-sWHWr"
      },
      "source": [
        "def modelInputs():\n",
        "    inputs=tf.placeholder(tf.int32,[None,None],name='input') # arguments: type, size(matrix: size of batch + sequence length), name\n",
        "    targets=tf.placeholder(tf.int32,[None,None],name='target')\n",
        "    lr=tf.placeholder(tf.float32,name='learning_rate')\n",
        "    keep_prob=tf.placeholder(tf.float32,name='drop_out_rate') #A hyperparameter that designate the dropout rate, generally it's at 20% (This idea helps prevent overfitting)\n",
        "    return(inputs,targets,lr,keep_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx3n-Jv-WHWr"
      },
      "source": [
        "As you know a RNN model is composed of two main parts, an encoder part that recieves the input sequence, and a decoder that generates sequentially the output. In Tensorflow the decode needs the targets in a particular form which is composed of two main phases. First we must provide targets by batches (a batch size to specified) and also to ensure every target (answer) of the batch starts with a '< SOS >' tag. So that's the plan of attack for the next step. Let's start.\n",
        "\n",
        "![image info](./EN-DE.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Y9h95NWHWr"
      },
      "source": [
        "def preprocess_targets(targets,word2int,batch_size): #word2in is the dictionary that maps words to their ID.\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UoKk5GBWHWs"
      },
      "source": [
        "# Now we will officially start the architecture of the model. So first the encoder:\n",
        "\n",
        "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length): #rnn_size is number of input tensors in the encoder/ list of length of sequences of the batch\n",
        "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) #create the LSTM\n",
        "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob) #Creating the the dropout\n",
        "    # Till now u just created the architecture of one cell of the RNN(LSTM). Now to create the encoder cell.\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
        "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell,\n",
        "                                                                    cell_bw = encoder_cell,\n",
        "                                                                    sequence_length = sequence_length,\n",
        "                                                                    inputs = rnn_inputs,\n",
        "                                                                    dtype = tf.float32)\n",
        "    # Making the chatbot as good as we can by using a bidirectional RNN.\n",
        "    return encoder_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzNCzUxuWHWt"
      },
      "source": [
        "# Now we will implement the fucntion that does the decoding for the training set then returns the decoding\n",
        "# outputs, we also implemented the attention concept. \n",
        "\n",
        "def decode_trainSet(encoder_state,decoder_cell,decoder_embedded_inputs,sequence_length,decoding_scope,output_function,keep_prob,batch_size): #Embeddings are representations of words in a unique vector of numbers, in our case thery are the inputs for the decoder\n",
        "    attention_states=tf.zeros([batch_size,1,decoder_cell.output_size])\n",
        "    attention_keys,attention_values,attention_score_function,attention_construct_function=tf.contrib.seq2seq.prepare_attention(attention_states,attention_option='bahdanau',num_units=decoder_cell.output_size)\n",
        "    training_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                           attention_keys,\n",
        "                                                                            attention_values,\n",
        "                                                                            attention_score_function,\n",
        "                                                                            attention_construct_function,\n",
        "                                                                           name='att_dec_train')\n",
        "    decoder_output,_,_=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,training_decoder_function,decoder_embedded_inputs,sequence_length,decoding_scope)\n",
        "    decoder_output_drop_out=tf.nn.dropout(decoder_output,keep_prob)\n",
        "    return(output_function(decoder_output_drop_out))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2b0WTq7WHWu"
      },
      "source": [
        "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              name = \"attn_dec_train\")\n",
        "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                                                              training_decoder_function,\n",
        "                                                                                                              decoder_embedded_input,\n",
        "                                                                                                              sequence_length,\n",
        "                                                                                                              scope = decoding_scope)\n",
        "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
        "    return output_function(decoder_output_dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5lq7snoWHWv"
      },
      "source": [
        "# Now for the decoder intended for the test/validation sets. This is going to be very similar to the last part.\n",
        "\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
        "                                                                              encoder_state[0],\n",
        "                                                                              attention_keys,\n",
        "                                                                              attention_values,\n",
        "                                                                              attention_score_function,\n",
        "                                                                              attention_construct_function,\n",
        "                                                                              decoder_embeddings_matrix,\n",
        "                                                                              sos_id,\n",
        "                                                                              eos_id,\n",
        "                                                                              maximum_length,\n",
        "                                                                              num_words,\n",
        "                                                                              name = \"attn_dec_inf\")\n",
        "    test_predictions,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
        "                                                                test_decoder_function,\n",
        "                                                                scope = decoding_scope)\n",
        "    return test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoWrqh2mWHWw"
      },
      "source": [
        "#Now at last we create the decoder\n",
        "\n",
        "def decoder_rnn(decoder_embedded_input,decoder_embeddings_matrix,encoder_state,num_words,sequence_length,rnn_size,num_layers,word2int,keep_prob,batch_size):\n",
        "    \n",
        "    with tf.variable_scope('decoding') as decoding_scope:\n",
        "        lstm=tf.contrib.rnn.BasicLSTMCell(rnn_size) #the following 3 lines are same as decoder\n",
        "        lstm_dropOut=tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
        "        decoder_cell=tf.contrib.rnn.MultiRNNCell([lstm_dropOut]*num_layers)\n",
        "        weights=tf.truncated_normal_initializer(stddev=0.1)\n",
        "        biases=tf.zeros_initializer()\n",
        "        output_function=lambda x: tf.contrib.layers.fully_connected(x,\n",
        "                                                                   num_words,\n",
        "                                                                   None,\n",
        "                                                                   scope=decoding_scope,\n",
        "                                                                   weights_initializer=weights,\n",
        "                                                                    biases_initializer=biases)\n",
        "        training_predictions=decode_training_set(encoder_state,decoder_cell,decoder_embedded_input,sequence_length,decoding_scope,output_function,keep_prob,batch_size)\n",
        "        \n",
        "        decoding_scope.reuse_variables()\n",
        "        test_predictions=decode_test_set(encoder_state,decoder_cell,decoder_embeddings_matrix,word2int['<SOS>'],word2int['<EOS>'],sequence_length-1,num_words,decoding_scope,output_function,keep_prob,batch_size)\n",
        "    return(training_predictions,test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xvcUYN4WHWx"
      },
      "source": [
        "# Building the SEQ2SEQ Model.\n",
        "\n",
        "def seq2seq_model(inputs,targets,keep_prob,batch_size,sequence_length,answers_num_words,questions_num_words,encoder_embedding_size,decoder_embedding_size,rnn_size,num_layers,questionwordsIDs):\n",
        "    encoder_embeded_input=tf.contrib.layers.embed_sequence(inputs,\n",
        "                                                          answers_num_words+1,\n",
        "                                                          encoder_embedding_size,\n",
        "                                                          initializer=tf.random_uniform_initializer(1,0))\n",
        "    encoder_state=encoder_rnn(encoder_embeded_input,rnn_size,num_layers,keep_prob,sequence_length)\n",
        "    preprocTargets=preprocess_targets(targets,questionwordsIDs,batch_size)\n",
        "    decoder_embeddings_matrix=tf.Variable(tf.random_uniform([questions_num_words+1,decoder_embedding_size],0,1))\n",
        "    decoder_embedded_input=tf.nn.embedding_lookup(decoder_embeddings_matrix,preprocTargets)\n",
        "    training_predictions,test_predictions=decoder_rnn(decoder_embedded_input,decoder_embeddings_matrix,encoder_state,questions_num_words,sequence_length,rnn_size,num_layers,questionwordsIDs,keep_prob,batch_size)\n",
        "    return(training_predictions,test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX_g10gXWHWx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0LPeDUdWHWx"
      },
      "source": [
        "# III. Training the SEQ2SEQ model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHAWWmh0WHWx"
      },
      "source": [
        "# We'll start by setting the hyperparameters thar will be used during the training. Obviously those are to be tweaked to make\n",
        "#the chatbot be better.\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 32 # it is advised to use a batch size as a power of 2.\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgW9bSx5WHWx"
      },
      "source": [
        "Tensorflow uses Dataflow programing which is a pradigm that models a program as a oriented graph, for which nodes are the operations and the  edges represent input and output data, this helps with parallelism which is important in Deep learning computations. So to use it we should first create a dataflow graph then create a session to run parts of the graph.\n",
        "\n",
        "So that's what we will do now defining the session for the training phase.\n",
        "\n",
        "Note: The only difference with a regular Session is that an InteractiveSession installs itself as the default session on construction. The methods Tensor.eval() and Operation.run() will use that session to run ops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3GnpFsVWHWy"
      },
      "source": [
        "tf.reset_default_graph() #reseting the tf default graph which we will use.\n",
        "session= tf.InteractiveSession() # Creating the interactive session."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1Vc1FLWHWy"
      },
      "source": [
        "# Loading the model inputs:\n",
        "\n",
        "inputs,targets,lr,keep_prob = modelInputs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uH1pXY8WHWy"
      },
      "source": [
        "# Setting the sequence length\n",
        "\n",
        "sequence_length=tf.placeholder_with_default(25,None,name='sequence_length')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ_xcxWSWHWy"
      },
      "source": [
        "# Setting the input shape\n",
        "\n",
        "input_shape=tf.shape(inputs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKDddGWoWHWy"
      },
      "source": [
        "# Now for the exciting stuff, by getting the train/test predictions.\n",
        "\n",
        "training_predictions,test_predictions = seq2seq_model(tf.reverse(inputs,[-1]),\n",
        "                                                      targets,\n",
        "                                                      keep_prob,\n",
        "                                                      batch_size,\n",
        "                                                      sequence_length,\n",
        "                                                      len(questionwordsIDs),\n",
        "                                                      len(answerwordsIDs),\n",
        "                                                      encoding_embedding_size,\n",
        "                                                      decoding_embedding_size,\n",
        "                                                      rnn_size,\n",
        "                                                      num_layers,\n",
        "                                                      questionwordsIDs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPG-1KUiWHWz"
      },
      "source": [
        "# Setting loss error, optimizer and gradient clipping (Forcing gradient to a min/max values if it breaches the bounds).\n",
        "\n",
        "with tf.name_scope('Optimization'):\n",
        "    \n",
        "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,targets,tf.ones([input_shape[0],sequence_length]))\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate)\n",
        "    gradients=optimizer.compute_gradients(loss_error)\n",
        "    clipped_gradients=[(tf.clip_by_value(grad_tensor,-5.,5.),grad_var) for grad_tensor,grad_var in gradients if grad_tensor is not None]\n",
        "    oprimizer_gradient_clipping=optimizer.apply_gradients(clipped_gradients)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV-lVzB9WHWz"
      },
      "source": [
        "# Now we will apply padding. Which means completing a sentence with n words to reach m>n words using <PAD> tags. \n",
        "# this is important in the sense that questions and answers must have same length.\\\n",
        "\n",
        "def apply_padding(batch,word2int): #word2int dict maps a word to integer\n",
        "    max_seq=max([len(sequence) for sequence in batch])\n",
        "    return([seq+[word2int['<PAD>']]*(max_seq-len(seq)) for seq in batch])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zOcFxWkWHWz"
      },
      "source": [
        "# split data into batches of answer and questions\n",
        "\n",
        "def split_into_batches(questions,answers,batch_size):\n",
        "    Qnum_batch=len(answers)//batch_size\n",
        "    for i in range (Qnum_batch):\n",
        "        start=i * batch_size\n",
        "        Qbatch=questions[start:start+batch_size]\n",
        "        Abatch=answers[start:start+batch_size]\n",
        "        paddedQbatch=np.array(apply_padding(Qbatch,questionwordsIDs))\n",
        "        paddedAbatch=np.array(apply_padding(Abatch,answerwordsIDs))\n",
        "        yield  paddedQbatch,paddedAbatch\n",
        "# Notice that yield is inside the loop\n",
        "# Return sends a specified value back to its caller whereas Yield can produce a sequence of values. \n",
        "# We should use yield when we want to iterate over a sequence, but don’t want to store the entire sequence in memory.\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_THTdmqWHWz"
      },
      "source": [
        "# Splitting data (Q&A) into train/dev/test sets.\n",
        "\n",
        "train_val_split=int(len(SortclQues)*0.15)\n",
        "\n",
        "training_quest=SortclQues[train_val_split:]\n",
        "training_answ=SortclAns[train_val_split:]\n",
        "\n",
        "validation_quest=SortclQues[0:train_val_split]\n",
        "validation_answ=SortclAns[0:train_val_split]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "76snPXNrWHWz",
        "outputId": "3f0f774e-4e7a-4e92-c27b-8987c16aa3f5"
      },
      "source": [
        "# Training:\n",
        "\n",
        "batch_index_check_training_loss = 100\n",
        "batch_index_check_validation_loss = ((len(training_quest)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 1000\n",
        "checkpoint = \"./chatbot_weights.ckpt\" \n",
        "session.run(tf.global_variables_initializer())\n",
        "for epoch in range(1, epochs + 1):\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_quest, training_answ, batch_size)):\n",
        "        starting_time = time.time()\n",
        "        _, batch_training_loss_error = session.run([oprimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
        "                                                                                               targets: padded_answers_in_batch,\n",
        "                                                                                               lr: learning_rate,\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                                                keep_prob:0.5})\n",
        "        total_training_loss_error += batch_training_loss_error\n",
        "        ending_time = time.time()\n",
        "        batch_time = ending_time - starting_time\n",
        "        if batch_index % batch_index_check_training_loss == 0:\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_index,\n",
        "                                                                                                                                       len(training_quest) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "            \n",
        "            total_training_loss_error = 0\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
        "            total_validation_loss_error = 0\n",
        "            starting_time = time.time()\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_quest, validation_answ, batch_size)):\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
        "                                                                       targets: padded_answers_in_batch,\n",
        "                                                                       lr: learning_rate,\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
        "                                                                       keep_prob: 1})\n",
        "                total_validation_loss_error += batch_validation_loss_error\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_quest) / batch_size)\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "            #implementing learning rate decay.\n",
        "            learning_rate *= learning_rate_decay\n",
        "            if learning_rate < min_learning_rate:\n",
        "                learning_rate = min_learning_rate\n",
        "            # Now for early stopping:\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                print('I speak better now!!') # Meaning we improved validation loss error. It is smaller than before.\n",
        "                early_stopping_check = 0\n",
        "                saver = tf.train.Saver()\n",
        "                saver.save(session, checkpoint) #we defined checkpoint before\n",
        "            else:\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                early_stopping_check += 1\n",
        "                if early_stopping_check == early_stopping_stop:\n",
        "                    break\n",
        "    if early_stopping_check == early_stopping_stop:\n",
        "        print(\"Sorry! I can't speak better anymore, this is my limit!\")\n",
        "        break\n",
        "print(\"Game Over\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1/100, Batch:    0/4111, Training Loss Error:  0.088, Training Time on 100 Batches: 5029 seconds\n",
            "Epoch:   1/100, Batch:  100/4111, Training Loss Error:  2.930, Training Time on 100 Batches: 4529 seconds\n",
            "Epoch:   1/100, Batch:  200/4111, Training Loss Error:  2.316, Training Time on 100 Batches: 3851 seconds\n",
            "Epoch:   1/100, Batch:  300/4111, Training Loss Error:  2.285, Training Time on 100 Batches: 4166 seconds\n",
            "Epoch:   1/100, Batch:  400/4111, Training Loss Error:  2.207, Training Time on 100 Batches: 3870 seconds\n",
            "Epoch:   1/100, Batch:  500/4111, Training Loss Error:  2.180, Training Time on 100 Batches: 3769 seconds\n",
            "Epoch:   1/100, Batch:  600/4111, Training Loss Error:  2.202, Training Time on 100 Batches: 1412 seconds\n",
            "Epoch:   1/100, Batch:  700/4111, Training Loss Error:  2.164, Training Time on 100 Batches: 1478 seconds\n",
            "Epoch:   1/100, Batch:  800/4111, Training Loss Error:  2.156, Training Time on 100 Batches: 1438 seconds\n",
            "Epoch:   1/100, Batch:  900/4111, Training Loss Error:  2.164, Training Time on 100 Batches: 1563 seconds\n",
            "Epoch:   1/100, Batch: 1000/4111, Training Loss Error:  2.127, Training Time on 100 Batches: 1124 seconds\n",
            "Epoch:   1/100, Batch: 1100/4111, Training Loss Error:  2.113, Training Time on 100 Batches: 1454 seconds\n",
            "Epoch:   1/100, Batch: 1200/4111, Training Loss Error:  2.077, Training Time on 100 Batches: 1545 seconds\n",
            "Epoch:   1/100, Batch: 1300/4111, Training Loss Error:  2.094, Training Time on 100 Batches: 1470 seconds\n",
            "Epoch:   1/100, Batch: 1400/4111, Training Loss Error:  1.996, Training Time on 100 Batches: 1467 seconds\n",
            "Epoch:   1/100, Batch: 1500/4111, Training Loss Error:  2.041, Training Time on 100 Batches: 1613 seconds\n",
            "Epoch:   1/100, Batch: 1600/4111, Training Loss Error:  2.039, Training Time on 100 Batches: 1232 seconds\n",
            "Epoch:   1/100, Batch: 1700/4111, Training Loss Error:  2.045, Training Time on 100 Batches: 1238 seconds\n",
            "Epoch:   1/100, Batch: 1800/4111, Training Loss Error:  2.046, Training Time on 100 Batches: 1313 seconds\n",
            "Epoch:   1/100, Batch: 1900/4111, Training Loss Error:  2.006, Training Time on 100 Batches: 1324 seconds\n",
            "Epoch:   1/100, Batch: 2000/4111, Training Loss Error:  1.982, Training Time on 100 Batches: 1445 seconds\n",
            "Validation Loss Error:  1.971, Batch Validation Time: 2540 seconds\n",
            "I speak better now!!\n",
            "Epoch:   1/100, Batch: 2100/4111, Training Loss Error:  1.999, Training Time on 100 Batches: 1553 seconds\n",
            "Epoch:   1/100, Batch: 2200/4111, Training Loss Error:  2.071, Training Time on 100 Batches: 1582 seconds\n",
            "Epoch:   1/100, Batch: 2300/4111, Training Loss Error:  2.005, Training Time on 100 Batches: 1284 seconds\n",
            "Epoch:   1/100, Batch: 2400/4111, Training Loss Error:  2.004, Training Time on 100 Batches: 1496 seconds\n",
            "Epoch:   1/100, Batch: 2500/4111, Training Loss Error:  2.043, Training Time on 100 Batches: 1531 seconds\n",
            "Epoch:   1/100, Batch: 2600/4111, Training Loss Error:  2.010, Training Time on 100 Batches: 1588 seconds\n",
            "Epoch:   1/100, Batch: 2700/4111, Training Loss Error:  2.026, Training Time on 100 Batches: 1535 seconds\n",
            "Epoch:   1/100, Batch: 2800/4111, Training Loss Error:  2.027, Training Time on 100 Batches: 1770 seconds\n",
            "Epoch:   1/100, Batch: 2900/4111, Training Loss Error:  2.023, Training Time on 100 Batches: 1523 seconds\n",
            "Epoch:   1/100, Batch: 3000/4111, Training Loss Error:  2.040, Training Time on 100 Batches: 1553 seconds\n",
            "Epoch:   1/100, Batch: 3100/4111, Training Loss Error:  2.025, Training Time on 100 Batches: 1701 seconds\n",
            "Epoch:   1/100, Batch: 3200/4111, Training Loss Error:  1.986, Training Time on 100 Batches: 1751 seconds\n",
            "Epoch:   1/100, Batch: 3300/4111, Training Loss Error:  1.960, Training Time on 100 Batches: 1620 seconds\n",
            "Epoch:   1/100, Batch: 3400/4111, Training Loss Error:  1.990, Training Time on 100 Batches: 1774 seconds\n",
            "Epoch:   1/100, Batch: 3500/4111, Training Loss Error:  1.961, Training Time on 100 Batches: 1556 seconds\n",
            "Epoch:   1/100, Batch: 3600/4111, Training Loss Error:  1.955, Training Time on 100 Batches: 1706 seconds\n",
            "Epoch:   1/100, Batch: 3700/4111, Training Loss Error:  1.960, Training Time on 100 Batches: 2106 seconds\n",
            "Epoch:   1/100, Batch: 3800/4111, Training Loss Error:  1.951, Training Time on 100 Batches: 1888 seconds\n",
            "Epoch:   1/100, Batch: 3900/4111, Training Loss Error:  1.986, Training Time on 100 Batches: 1943 seconds\n",
            "Epoch:   1/100, Batch: 4000/4111, Training Loss Error:  1.986, Training Time on 100 Batches: 1937 seconds\n",
            "Epoch:   1/100, Batch: 4100/4111, Training Loss Error:  1.955, Training Time on 100 Batches: 2215 seconds\n",
            "Validation Loss Error:  1.902, Batch Validation Time: 2528 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch:    0/4111, Training Loss Error:  0.225, Training Time on 100 Batches: 1296 seconds\n",
            "Epoch:   2/100, Batch:  100/4111, Training Loss Error:  1.915, Training Time on 100 Batches: 1343 seconds\n",
            "Epoch:   2/100, Batch:  200/4111, Training Loss Error:  1.898, Training Time on 100 Batches: 1095 seconds\n",
            "Epoch:   2/100, Batch:  300/4111, Training Loss Error:  1.914, Training Time on 100 Batches: 1351 seconds\n",
            "Epoch:   2/100, Batch:  400/4111, Training Loss Error:  1.870, Training Time on 100 Batches: 1346 seconds\n",
            "Epoch:   2/100, Batch:  500/4111, Training Loss Error:  1.868, Training Time on 100 Batches: 1528 seconds\n",
            "Epoch:   2/100, Batch:  600/4111, Training Loss Error:  1.896, Training Time on 100 Batches: 1440 seconds\n",
            "Epoch:   2/100, Batch:  700/4111, Training Loss Error:  1.880, Training Time on 100 Batches: 1468 seconds\n",
            "Epoch:   2/100, Batch:  800/4111, Training Loss Error:  1.897, Training Time on 100 Batches: 1396 seconds\n",
            "Epoch:   2/100, Batch:  900/4111, Training Loss Error:  1.918, Training Time on 100 Batches: 1484 seconds\n",
            "Epoch:   2/100, Batch: 1000/4111, Training Loss Error:  1.901, Training Time on 100 Batches: 1170 seconds\n",
            "Epoch:   2/100, Batch: 1100/4111, Training Loss Error:  1.908, Training Time on 100 Batches: 1462 seconds\n",
            "Epoch:   2/100, Batch: 1200/4111, Training Loss Error:  1.889, Training Time on 100 Batches: 1595 seconds\n",
            "Epoch:   2/100, Batch: 1300/4111, Training Loss Error:  1.911, Training Time on 100 Batches: 1523 seconds\n",
            "Epoch:   2/100, Batch: 1400/4111, Training Loss Error:  1.833, Training Time on 100 Batches: 1607 seconds\n",
            "Epoch:   2/100, Batch: 1500/4111, Training Loss Error:  1.884, Training Time on 100 Batches: 1518 seconds\n",
            "Epoch:   2/100, Batch: 1600/4111, Training Loss Error:  1.884, Training Time on 100 Batches: 1240 seconds\n",
            "Epoch:   2/100, Batch: 1700/4111, Training Loss Error:  1.893, Training Time on 100 Batches: 1249 seconds\n",
            "Epoch:   2/100, Batch: 1800/4111, Training Loss Error:  1.902, Training Time on 100 Batches: 1449 seconds\n",
            "Epoch:   2/100, Batch: 1900/4111, Training Loss Error:  1.870, Training Time on 100 Batches: 1376 seconds\n",
            "Epoch:   2/100, Batch: 2000/4111, Training Loss Error:  1.852, Training Time on 100 Batches: 1446 seconds\n",
            "Validation Loss Error:  1.840, Batch Validation Time: 2527 seconds\n",
            "I speak better now!!\n",
            "Epoch:   2/100, Batch: 2100/4111, Training Loss Error:  1.873, Training Time on 100 Batches: 1573 seconds\n",
            "Epoch:   2/100, Batch: 2200/4111, Training Loss Error:  1.945, Training Time on 100 Batches: 1521 seconds\n",
            "Epoch:   2/100, Batch: 2300/4111, Training Loss Error:  1.888, Training Time on 100 Batches: 1278 seconds\n",
            "Epoch:   2/100, Batch: 2400/4111, Training Loss Error:  1.889, Training Time on 100 Batches: 1604 seconds\n",
            "Epoch:   2/100, Batch: 2500/4111, Training Loss Error:  1.927, Training Time on 100 Batches: 1546 seconds\n",
            "Epoch:   2/100, Batch: 2600/4111, Training Loss Error:  1.902, Training Time on 100 Batches: 1490 seconds\n",
            "Epoch:   2/100, Batch: 2700/4111, Training Loss Error:  1.916, Training Time on 100 Batches: 1573 seconds\n",
            "Epoch:   2/100, Batch: 2800/4111, Training Loss Error:  1.922, Training Time on 100 Batches: 1763 seconds\n",
            "Epoch:   2/100, Batch: 2900/4111, Training Loss Error:  1.917, Training Time on 100 Batches: 1667 seconds\n",
            "Epoch:   2/100, Batch: 3000/4111, Training Loss Error:  1.938, Training Time on 100 Batches: 1609 seconds\n",
            "Epoch:   2/100, Batch: 3100/4111, Training Loss Error:  1.925, Training Time on 100 Batches: 1688 seconds\n",
            "Epoch:   2/100, Batch: 3200/4111, Training Loss Error:  1.890, Training Time on 100 Batches: 1753 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:   2/100, Batch: 3300/4111, Training Loss Error:  1.865, Training Time on 100 Batches: 1628 seconds\n",
            "Epoch:   2/100, Batch: 3400/4111, Training Loss Error:  1.898, Training Time on 100 Batches: 1779 seconds\n",
            "Epoch:   2/100, Batch: 3500/4111, Training Loss Error:  1.873, Training Time on 100 Batches: 1599 seconds\n",
            "Epoch:   2/100, Batch: 3600/4111, Training Loss Error:  1.863, Training Time on 100 Batches: 1656 seconds\n",
            "Epoch:   2/100, Batch: 3700/4111, Training Loss Error:  1.876, Training Time on 100 Batches: 1768 seconds\n",
            "Epoch:   2/100, Batch: 3800/4111, Training Loss Error:  1.869, Training Time on 100 Batches: 2123 seconds\n",
            "Epoch:   2/100, Batch: 3900/4111, Training Loss Error:  1.899, Training Time on 100 Batches: 2096 seconds\n",
            "Epoch:   2/100, Batch: 4000/4111, Training Loss Error:  1.904, Training Time on 100 Batches: 1913 seconds\n",
            "Epoch:   2/100, Batch: 4100/4111, Training Loss Error:  1.876, Training Time on 100 Batches: 2096 seconds\n",
            "Validation Loss Error:  1.839, Batch Validation Time: 2545 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch:    0/4111, Training Loss Error:  0.216, Training Time on 100 Batches: 1329 seconds\n",
            "Epoch:   3/100, Batch:  100/4111, Training Loss Error:  1.843, Training Time on 100 Batches: 1362 seconds\n",
            "Epoch:   3/100, Batch:  200/4111, Training Loss Error:  1.826, Training Time on 100 Batches: 1115 seconds\n",
            "Epoch:   3/100, Batch:  300/4111, Training Loss Error:  1.847, Training Time on 100 Batches: 1270 seconds\n",
            "Epoch:   3/100, Batch:  400/4111, Training Loss Error:  1.803, Training Time on 100 Batches: 1346 seconds\n",
            "Epoch:   3/100, Batch:  500/4111, Training Loss Error:  1.801, Training Time on 100 Batches: 1463 seconds\n",
            "Epoch:   3/100, Batch:  600/4111, Training Loss Error:  1.829, Training Time on 100 Batches: 1442 seconds\n",
            "Epoch:   3/100, Batch:  700/4111, Training Loss Error:  1.817, Training Time on 100 Batches: 1574 seconds\n",
            "Epoch:   3/100, Batch:  800/4111, Training Loss Error:  1.830, Training Time on 100 Batches: 1567 seconds\n",
            "Epoch:   3/100, Batch:  900/4111, Training Loss Error:  1.848, Training Time on 100 Batches: 1484 seconds\n",
            "Epoch:   3/100, Batch: 1000/4111, Training Loss Error:  1.837, Training Time on 100 Batches: 1323 seconds\n",
            "Epoch:   3/100, Batch: 1100/4111, Training Loss Error:  1.844, Training Time on 100 Batches: 1468 seconds\n",
            "Epoch:   3/100, Batch: 1200/4111, Training Loss Error:  1.829, Training Time on 100 Batches: 1537 seconds\n",
            "Epoch:   3/100, Batch: 1300/4111, Training Loss Error:  1.847, Training Time on 100 Batches: 1510 seconds\n",
            "Epoch:   3/100, Batch: 1400/4111, Training Loss Error:  1.776, Training Time on 100 Batches: 1604 seconds\n",
            "Epoch:   3/100, Batch: 1500/4111, Training Loss Error:  1.825, Training Time on 100 Batches: 1620 seconds\n",
            "Epoch:   3/100, Batch: 1600/4111, Training Loss Error:  1.824, Training Time on 100 Batches: 1317 seconds\n",
            "Epoch:   3/100, Batch: 1700/4111, Training Loss Error:  1.836, Training Time on 100 Batches: 1285 seconds\n",
            "Epoch:   3/100, Batch: 1800/4111, Training Loss Error:  1.848, Training Time on 100 Batches: 1396 seconds\n",
            "Epoch:   3/100, Batch: 1900/4111, Training Loss Error:  1.811, Training Time on 100 Batches: 1518 seconds\n",
            "Epoch:   3/100, Batch: 2000/4111, Training Loss Error:  1.796, Training Time on 100 Batches: 1582 seconds\n",
            "Validation Loss Error:  1.794, Batch Validation Time: 2525 seconds\n",
            "I speak better now!!\n",
            "Epoch:   3/100, Batch: 2100/4111, Training Loss Error:  1.818, Training Time on 100 Batches: 1784 seconds\n",
            "Epoch:   3/100, Batch: 2200/4111, Training Loss Error:  1.886, Training Time on 100 Batches: 1685 seconds\n",
            "Epoch:   3/100, Batch: 2300/4111, Training Loss Error:  1.834, Training Time on 100 Batches: 1365 seconds\n",
            "Epoch:   3/100, Batch: 2400/4111, Training Loss Error:  1.835, Training Time on 100 Batches: 1585 seconds\n",
            "Epoch:   3/100, Batch: 2500/4111, Training Loss Error:  1.869, Training Time on 100 Batches: 1617 seconds\n",
            "Epoch:   3/100, Batch: 2600/4111, Training Loss Error:  1.853, Training Time on 100 Batches: 1560 seconds\n",
            "Epoch:   3/100, Batch: 2700/4111, Training Loss Error:  1.860, Training Time on 100 Batches: 1654 seconds\n",
            "Epoch:   3/100, Batch: 2800/4111, Training Loss Error:  1.869, Training Time on 100 Batches: 1859 seconds\n",
            "Epoch:   3/100, Batch: 2900/4111, Training Loss Error:  1.865, Training Time on 100 Batches: 1635 seconds\n",
            "Epoch:   3/100, Batch: 3000/4111, Training Loss Error:  1.887, Training Time on 100 Batches: 1838 seconds\n",
            "Epoch:   3/100, Batch: 3100/4111, Training Loss Error:  1.872, Training Time on 100 Batches: 1696 seconds\n",
            "Epoch:   3/100, Batch: 3200/4111, Training Loss Error:  1.839, Training Time on 100 Batches: 1923 seconds\n",
            "Epoch:   3/100, Batch: 3300/4111, Training Loss Error:  1.819, Training Time on 100 Batches: 1796 seconds\n",
            "Epoch:   3/100, Batch: 3400/4111, Training Loss Error:  1.850, Training Time on 100 Batches: 2242 seconds\n",
            "Epoch:   3/100, Batch: 3500/4111, Training Loss Error:  1.825, Training Time on 100 Batches: 1902 seconds\n",
            "Epoch:   3/100, Batch: 3600/4111, Training Loss Error:  1.818, Training Time on 100 Batches: 1788 seconds\n",
            "Epoch:   3/100, Batch: 3700/4111, Training Loss Error:  1.826, Training Time on 100 Batches: 2245 seconds\n",
            "Epoch:   3/100, Batch: 3800/4111, Training Loss Error:  1.825, Training Time on 100 Batches: 2109 seconds\n",
            "Epoch:   3/100, Batch: 3900/4111, Training Loss Error:  1.854, Training Time on 100 Batches: 2468 seconds\n",
            "Epoch:   3/100, Batch: 4000/4111, Training Loss Error:  1.858, Training Time on 100 Batches: 2085 seconds\n",
            "Epoch:   3/100, Batch: 4100/4111, Training Loss Error:  1.829, Training Time on 100 Batches: 2317 seconds\n",
            "Validation Loss Error:  1.799, Batch Validation Time: 2532 seconds\n",
            "Sorry I do not speak better, I need to practice more.\n",
            "Epoch:   4/100, Batch:    0/4111, Training Loss Error:  0.210, Training Time on 100 Batches: 1443 seconds\n",
            "Epoch:   4/100, Batch:  100/4111, Training Loss Error:  1.801, Training Time on 100 Batches: 1553 seconds\n",
            "Epoch:   4/100, Batch:  200/4111, Training Loss Error:  1.785, Training Time on 100 Batches: 1245 seconds\n",
            "Epoch:   4/100, Batch:  300/4111, Training Loss Error:  1.805, Training Time on 100 Batches: 1471 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}